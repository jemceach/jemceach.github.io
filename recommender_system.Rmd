---
title: 'Content-Based Recommender System'
output: 
  html_document:
    theme: yeti
    highlight: zenburn
    df_print: paged
    code_folding: hide
---

<div>
<p>
Content-based recommender systems are used to filter user recommendations based on unique, item profiles. The recommender built below was modified from <a href="https://github.com/jemceach/recommender-systems/blob/master/project-2/report.ipynb" target="_blank">academic coursework</a>, which used Goodreads book ratings obtained from <a href="https://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/" target="_blank">FastML</a>. For replication purposes, data can be obtained from either the FastML website or the original github repository:  
<ul>
  <li>
    <a href="https://raw.githubusercontent.com/jemceach/recommender-systems/master/project-2/data/books.csv" target="_blank">Books Data</a>
  </li>
  <li>
  <a href="https://raw.githubusercontent.com/jemceach/recommender-systems/master/project-2/data/book_tags.csv"  target="_blank">Tags ID</a>
  </li>
  <li>
  <a href="https://raw.githubusercontent.com/jemceach/recommender-systems/master/project-2/data/tags.csv"  target="_blank">Tags Data</a>
  </li>
</ul>
</p>

<h3>Getting Started</h3>

```{r set-up, echo=F, cache=F, comment=F, message=F, warning=F, tidy=T}
# Libraries 
library(knitr); library(kableExtra)

# Set default augments for code chunks
knitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, comment=NA, fig.width=10, fig.height = 3, out.width='100%', tidy=F, tidy.opts=options(width=100))
```

<p>Building a recommender from this data first requires a little data preperation. Transformations were applied to the books data to subset columns and select english-language books from the dataset. Tags were combined and also tidied to remove numbers and punctuation.</p>
<p>Next, the item profiles for books were built by joining the cleaned book and tags data. These tags are a very important feature for the content-based recommender we are building. They contain the content for each item profile. </p>

```{r df_books}
library(tidyverse)

# Load Data 
load('./projects/data/recommender-system.RData')

# Clean books data
tidy_books <- book_data %>%
  ## subset columns
  select(title, 
         authors,
         average_rating,
         goodreads_book_id, 
         original_publication_year, 
         language_code) %>%
  ## filter books based on language (english)
  filter(grepl('en', language_code)) %>%
  ## drop observations with no publication year
  filter(!is.na(original_publication_year)) %>%
  ## mutate data types
  mutate(goodreads_book_id = as.integer(goodreads_book_id),
         authors = as.character(authors),
         title = as.character(title),
         language_code = as.character(language_code)) %>%
  ## Select items for item profile
  select(goodreads_book_id, title, authors) 

# Clean Tags data
tidy_tags <- tags_id %>%
  ## join tag_ids using book id
  left_join(tags_data, by = "tag_id") %>%
  # Find/remove numbers and punctuation
  mutate(tag_name = gsub("[0-9]|[[:punct:]]", " ", tag_name)) %>%
  # Filter empty columns containing only whitespace
  filter(grepl('\\w', tag_name)) %>%
  ## Drop tag columns no longer needed 
  select(-tag_id, -count)

# Build Item Profile
item_profile <- tidy_books %>% 
  ## join tags data using tag ids 
  left_join(tidy_tags, by = "goodreads_book_id") %>% 
  ## Mutate data type of tag name variable 
  mutate(tag_name = as.character(tag_name))  %>%
  ## Rename for simplicity
  rename(tags = tag_name, id = goodreads_book_id) 
```

<h4>Item Profile Exerpt: </h4>

```{r item_profile, echo=F}
# Preview Item Profile
item_profile %>% filter(id == 1)
```


```{r, eval=F, echo=F}
# Preview entry
item_profile%>%
  ## Group by book id 
  group_by(id) %>%
  filter(id == 1) %>%
  ## Concatenate tags
  mutate(tags = paste0(tags, collapse = ", "))  %>%
  ungroup() %>%
  unique() %>% 
  arrange(id) %>% t() %>% kable(caption = "Item-Profile Example") %>% kable_styling() %>% column_spec(1, bold=T)
```


<h3>Term Frequency - Inverse Document Frequency</h3>

<p>
Term Frequency *times* Inverse Document Frequency (TF-IDF) uses text-mining to score important words by occurance. We can apply this process to our item profile to evaluate tag importance for each book and generate recommendations.
</p>

<h4>Term Frequency</h4>
<p>
First, let's look at term frequency (TF) using a document-term matrix (DTM). A DTM evaluates the frequency of terms per document within a corpus of text. We can construct a DTM from the `item_profile` dataframe to analyse the text within each book tag. 
</p>
<p>
Using the **`tidytext`** package, each row was unnested by word and then further processed with the **`tm` ** and **`SnowballC`** packages. Common, stop words such as "and", were removed.  Words were also stemmed to obtain the base form of each word. An example of this would be stemming the word, "winning", to "win".
</p>
<p>
The frequency of each word per book id was then evaluated, tallied, and casted into the DTM. 
</p>
```{r dtm}
library(tidytext); library(tm); library(SnowballC)

# Document-term Matrix 
dtm <- item_profile %>%
  ## Split tag corpa into tokens  
  tidytext::unnest_tokens(token = "words", tags, tags) %>%
  ## Tidy: Remove stop words
  filter(!tags %in% tm::stopwords(kind = "english")) %>%
  ## Tidy: Stem words
  mutate_at("tags", funs(SnowballC::wordStem((.), language="english"))) %>%
  ## Count frequency of tags
  count(id, tags) %>%
  ## Cast tidied dataframe into DocumentTermMatrix
  tidytext::cast_dtm(term = tags,document = id,  value = n) %>% 
  ## Remove sparse terms with maximum sparisity of 90%
  tm::removeSparseTerms(.9)

# Inspect sample from DTM
tm::inspect(dtm)
```

<h4> Inverse Document Frequency </h4>

<p>The inverse document frequency (IDF) component informs how useful a term is. This is calculated by taking the logarithm of the total the number of terms within the corpus divided the total by the number of documents were an individual term appears. The **`tm`** package can be used again efficiently weight the sparse, document term matrix by calculating and multiplying the TF and IDF matrices. </p>

```{r tfidf}
tfidf <- tm::weightTfIdf(dtm, normalize = T)
```

<h3>Text Clustering</h3>
<p>Text clustering is used to understand and categorize text-based documents, such as the terms in our TF-IDF matrix. We can cluster the data by applying cosine similarity to the TF-IDF matrix to evaluate the distance between document terms. The corresponding values provide a measurement of similarity between the terms. </p>

<p>While **`tm`** was useful for large-scale TF-IDF matrix calculation, this method results in a sparse, simple_triplet_matrix. The cosine similiary is calculated below using the slam package to properly interpret this data type.</p>

```{r}
library(slam)
co_sim <- 1 - crossprod_simple_triplet_matrix(tfidf)/(sqrt(col_sums(tfidf^2) %*% t(col_sums(tfidf^2))))

```






</div>
